# -*- coding: utf-8 -*-
"""Lab2 Cats vs Dogs.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1n41Jf7eAoqgk9dWt3W6hYcDJLWdPLqHA

# Lab 2: Cats vs Dogs

In this lab, you will train a convolutional neural network to classify an image
into one of two classes: "cat" or "dog". The code for the neural networks
you train will be written for you, and you are not (yet!) expected
to understand all provided code. However, by the end of the lab,
you should be able to:

1. Understand at a high level the training loop for a machine learning model.
2. Understand the distinction between training, validation, and test data.
3. The concepts of overfitting and underfitting.
4. Investigate how different hyperparameters, such as learning rate and batch size, affect the success of training.
5. Compare an ANN (aka Multi-Layer Perceptron) with a CNN.

### What to submit

Submit a PDF file containing all your code, outputs, and write-up
from parts 1-5. You can produce a PDF of your Google Colab file by
going to **File > Print** and then save as PDF. The Colab instructions
has more information.

**Do not submit any other files produced by your code.**

Include a link to your colab file in your submission.

Please use Google Colab to complete this assignment. If you want to use Jupyter Notebook, please complete the assignment and upload your Jupyter Notebook file to Google Colab for submission.

With Colab, you can export a PDF file using the menu option
`File -> Print` and save as PDF file. **Adjust the scaling to ensure that the text is not cutoff at the margins.**

## Colab Link

Include a link to your colab file here

Colab Link: https://colab.research.google.com/drive/1n41Jf7eAoqgk9dWt3W6hYcDJLWdPLqHA?usp=sharing
"""

import numpy as np
import time
import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import torchvision
from torch.utils.data.sampler import SubsetRandomSampler
import torchvision.transforms as transforms

"""## Part 0. Helper Functions

We will be making use of the following helper functions. You will be asked to look
at and possibly modify some of these, but you are not expected to understand all of them.

You should look at the function names and read the docstrings. If you are curious, come back and explore the code *after* making some progress on the lab.
"""

###############################################################################
# Data Loading

def get_relevant_indices(dataset, classes, target_classes):
    """ Return the indices for datapoints in the dataset that belongs to the
    desired target classes, a subset of all possible classes.

    Args:
        dataset: Dataset object
        classes: A list of strings denoting the name of each class
        target_classes: A list of strings denoting the name of desired classes
                        Should be a subset of the 'classes'
    Returns:
        indices: list of indices that have labels corresponding to one of the
                 target classes
    """
    indices = []
    for i in range(len(dataset)):
        # Check if the label is in the target classes
        label_index = dataset[i][1] # ex: 3
        label_class = classes[label_index] # ex: 'cat'
        if label_class in target_classes:
            indices.append(i)
    return indices

def get_data_loader(target_classes, batch_size):
    """ Loads images of cats and dogs, splits the data into training, validation
    and testing datasets. Returns data loaders for the three preprocessed datasets.

    Args:
        target_classes: A list of strings denoting the name of the desired
                        classes. Should be a subset of the argument 'classes'
        batch_size: A int representing the number of samples per batch

    Returns:
        train_loader: iterable training dataset organized according to batch size
        val_loader: iterable validation dataset organized according to batch size
        test_loader: iterable testing dataset organized according to batch size
        classes: A list of strings denoting the name of each class
    """

    classes = ('plane', 'car', 'bird', 'cat',
               'deer', 'dog', 'frog', 'horse', 'ship', 'truck')
    ########################################################################
    # The output of torchvision datasets are PILImage images of range [0, 1].
    # We transform them to Tensors of normalized range [-1, 1].
    transform = transforms.Compose(
        [transforms.ToTensor(),
         transforms.Normalize((0.5, 0.5, 0.5), (0.5, 0.5, 0.5))])
    # Load CIFAR10 training data
    trainset = torchvision.datasets.CIFAR10(root='./data', train=True,
                                            download=True, transform=transform)
    # Get the list of indices to sample from
    relevant_indices = get_relevant_indices(trainset, classes, target_classes)

    # Split into train and validation
    np.random.seed(1000) # Fixed numpy random seed for reproducible shuffling
    np.random.shuffle(relevant_indices)
    split = int(len(relevant_indices) * 0.8) #split at 80%

    # split into training and validation indices
    relevant_train_indices, relevant_val_indices = relevant_indices[:split], relevant_indices[split:]
    train_sampler = SubsetRandomSampler(relevant_train_indices)
    train_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                               num_workers=1, sampler=train_sampler)
    val_sampler = SubsetRandomSampler(relevant_val_indices)
    val_loader = torch.utils.data.DataLoader(trainset, batch_size=batch_size,
                                              num_workers=1, sampler=val_sampler)
    # Load CIFAR10 testing data
    testset = torchvision.datasets.CIFAR10(root='./data', train=False,
                                           download=True, transform=transform)
    # Get the list of indices to sample from
    relevant_test_indices = get_relevant_indices(testset, classes, target_classes)
    test_sampler = SubsetRandomSampler(relevant_test_indices)
    test_loader = torch.utils.data.DataLoader(testset, batch_size=batch_size,
                                             num_workers=1, sampler=test_sampler)
    return train_loader, val_loader, test_loader, classes

###############################################################################
# Training
def get_model_name(name, batch_size, learning_rate, epoch):
    """ Generate a name for the model consisting of all the hyperparameter values

    Args:
        config: Configuration object containing the hyperparameters
    Returns:
        path: A string with the hyperparameter name and value concatenated
    """
    path = "model_{0}_bs{1}_lr{2}_epoch{3}".format(name,
                                                   batch_size,
                                                   learning_rate,
                                                   epoch)
    return path

def normalize_label(labels):
    """
    Given a tensor containing 2 possible values, normalize this to 0/1

    Args:
        labels: a 1D tensor containing two possible scalar values
    Returns:
        A tensor normalize to 0/1 value
    """
    max_val = torch.max(labels)
    min_val = torch.min(labels)
    norm_labels = (labels - min_val)/(max_val - min_val)
    return norm_labels

def evaluate(net, loader, criterion):
    """ Evaluate the network on the validation set.

     Args:
         net: PyTorch neural network object
         loader: PyTorch data loader for the validation set
         criterion: The loss function
     Returns:
         err: A scalar for the avg classification error over the validation set
         loss: A scalar for the average loss function over the validation set
     """
    total_loss = 0.0
    total_err = 0.0
    total_epoch = 0
    for i, data in enumerate(loader, 0):
        inputs, labels = data
        labels = normalize_label(labels)  # Convert labels to 0/1
        outputs = net(inputs)
        loss = criterion(outputs, labels.float())
        corr = (outputs > 0.0).squeeze().long() != labels
        total_err += int(corr.sum())
        total_loss += loss.item()
        total_epoch += len(labels)
    err = float(total_err) / total_epoch
    loss = float(total_loss) / (i + 1)
    return err, loss

###############################################################################
# Training Curve
def plot_training_curve(path):
    """ Plots the training curve for a model run, given the csv files
    containing the train/validation error/loss.

    Args:
        path: The base path of the csv files produced during training
    """
    import matplotlib.pyplot as plt
    train_err = np.loadtxt("{}_train_err.csv".format(path))
    val_err = np.loadtxt("{}_val_err.csv".format(path))
    train_loss = np.loadtxt("{}_train_loss.csv".format(path))
    val_loss = np.loadtxt("{}_val_loss.csv".format(path))
    plt.title("Train vs Validation Error")
    n = len(train_err) # number of epochs
    plt.plot(range(1,n+1), train_err, label="Train")
    plt.plot(range(1,n+1), val_err, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Error")
    plt.legend(loc='best')
    plt.show()
    plt.title("Train vs Validation Loss")
    plt.plot(range(1,n+1), train_loss, label="Train")
    plt.plot(range(1,n+1), val_loss, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend(loc='best')
    plt.show()

"""## Part 1. Visualizing the Data [7 pt]

We will make use of some of the CIFAR-10 data set, which consists of
colour images of size 32x32 pixels belonging to 10 categories. You can
find out more about the dataset at https://www.cs.toronto.edu/~kriz/cifar.html

For this assignment, we will only be using the cat and dog categories.
We have included code that automatically downloads the dataset the
first time that the main script is run.
"""

# This will download the CIFAR-10 dataset to a folder called "data"
# the first time you run this code.
train_loader, val_loader, test_loader, classes = get_data_loader(
    target_classes=["cat", "dog"],
    batch_size=1) # One image per batch

"""### Part (a) -- 1 pt

Visualize some of the data by running the code below.
Include the visualization in your writeup.

(You don't need to submit anything else.)
"""

import matplotlib.pyplot as plt

k = 0
for images, labels in train_loader:
    # since batch_size = 1, there is only 1 image in `images`
    image = images[0]
    # place the colour channel at the end, instead of at the beginning
    img = np.transpose(image, [1,2,0])
    # normalize pixel intensity values to [0, 1]
    img = img / 2 + 0.5
    plt.subplot(3, 5, k+1)
    plt.axis('off')
    plt.imshow(img)

    k += 1
    if k > 14:
        break

"""### Part (b) -- 3 pt

How many training examples do we have for the combined `cat` and `dog` classes?
What about validation examples?
What about test examples?
"""

# Number of training examples
num_train_examples = len(train_loader)
# Number of validation examples
num_val_examples = len(val_loader)
# Number of test examples
num_test_examples = len(test_loader)

print(f"Number of training examples: {num_train_examples}")
print(f"Number of validation examples: {num_val_examples}")
print(f"Number of test examples: {num_test_examples}")

"""### Part (c) -- 3pt

Why do we need a validation set when training our model? What happens if we judge the
performance of our models using the training set loss/error instead of the validation
set loss/error?

A validation set is essential for assessing a model's ability to generalize to new data and for tuning hyperparameters without risking overfitting. If you only use the training set loss/error to judge performance, you risk overfitting the model to the training data, resulting in poor performance on new, unseen data.

## Part 2. Training [15 pt]

We define two neural networks, a `LargeNet` and `SmallNet`.
We'll be training the networks in this section.

You won't understand fully what these networks are doing until
the next few classes, and that's okay. For this assignment, please
focus on learning how to train networks, and how hyperparameters affect
training.
"""

class LargeNet(nn.Module):
    def __init__(self):
        super(LargeNet, self).__init__()
        self.name = "large"
        self.conv1 = nn.Conv2d(3, 5, 5)
        self.pool = nn.MaxPool2d(2, 2)
        self.conv2 = nn.Conv2d(5, 10, 5)
        self.fc1 = nn.Linear(10 * 5 * 5, 32)
        self.fc2 = nn.Linear(32, 1)

    def forward(self, x):
        x = self.pool(F.relu(self.conv1(x)))
        x = self.pool(F.relu(self.conv2(x)))
        x = x.view(-1, 10 * 5 * 5)
        x = F.relu(self.fc1(x))
        x = self.fc2(x)
        x = x.squeeze(1) # Flatten to [batch_size]
        return x

class SmallNet(nn.Module):
    def __init__(self):
        super(SmallNet, self).__init__()
        self.name = "small"
        self.conv = nn.Conv2d(3, 5, 3)
        self.pool = nn.MaxPool2d(2, 2)
        self.fc = nn.Linear(5 * 7 * 7, 1)

    def forward(self, x):
        x = self.pool(F.relu(self.conv(x)))
        x = self.pool(x)
        x = x.view(-1, 5 * 7 * 7)
        x = self.fc(x)
        x = x.squeeze(1) # Flatten to [batch_size]
        return x

small_net = SmallNet()
large_net = LargeNet()

"""### Part (a) -- 2pt

The methods `small_net.parameters()` and `large_net.parameters()`
produces an iterator of all the trainable parameters of the network.
These parameters are torch tensors containing many scalar values.

We haven't learned how how the parameters in these high-dimensional
tensors will be used, but we should be able to count the number
of parameters. Measuring the number of parameters in a network is
one way of measuring the "size" of a network.

What is the total number of parameters in `small_net` and in
`large_net`? (Hint: how many numbers are in each tensor?)
"""

small_net_params = 0
for param in small_net.parameters():
    small_net_params += param.numel()
print(f"Total number of parameters in SmallNet: {small_net_params}")

large_net_params = 0
for param in large_net.parameters():
    large_net_params += param.numel()
print(f"Total number of parameters in LargeNet: {large_net_params}")

"""### The function train_net

The function `train_net` below takes an untrained neural network (like `small_net` and `large_net`) and
several other parameters. You should be able to understand how this function works.
The figure below shows the high level training loop for a machine learning model:

![alt text](https://github.com/UTNeural/Lab2/blob/master/Diagram.png?raw=true)
"""

def train_net(net, batch_size=64, learning_rate=0.01, num_epochs=30):
    ########################################################################
    # Train a classifier on cats vs dogs
    target_classes = ["cat", "dog"]
    ########################################################################
    # Fixed PyTorch random seed for reproducible result
    torch.manual_seed(1000)
    ########################################################################
    # Obtain the PyTorch data loader objects to load batches of the datasets
    train_loader, val_loader, test_loader, classes = get_data_loader(
            target_classes, batch_size)
    ########################################################################
    # Define the Loss function and optimizer
    # The loss function will be Binary Cross Entropy (BCE). In this case we
    # will use the BCEWithLogitsLoss which takes unnormalized output from
    # the neural network and scalar label.
    # Optimizer will be SGD with Momentum.
    criterion = nn.BCEWithLogitsLoss()
    optimizer = optim.SGD(net.parameters(), lr=learning_rate, momentum=0.9)
    ########################################################################
    # Set up some numpy arrays to store the training/test loss/erruracy
    train_err = np.zeros(num_epochs)
    train_loss = np.zeros(num_epochs)
    val_err = np.zeros(num_epochs)
    val_loss = np.zeros(num_epochs)
    ########################################################################
    # Train the network
    # Loop over the data iterator and sample a new batch of training data
    # Get the output from the network, and optimize our loss function.
    start_time = time.time()
    for epoch in range(num_epochs):  # loop over the dataset multiple times
        total_train_loss = 0.0
        total_train_err = 0.0
        total_epoch = 0
        for i, data in enumerate(train_loader, 0):
            # Get the inputs
            inputs, labels = data
            labels = normalize_label(labels) # Convert labels to 0/1
            # Zero the parameter gradients
            optimizer.zero_grad()
            # Forward pass, backward pass, and optimize
            outputs = net(inputs)
            loss = criterion(outputs, labels.float())
            loss.backward()
            optimizer.step()
            # Calculate the statistics
            corr = (outputs > 0.0).squeeze().long() != labels
            total_train_err += int(corr.sum())
            total_train_loss += loss.item()
            total_epoch += len(labels)
        train_err[epoch] = float(total_train_err) / total_epoch
        train_loss[epoch] = float(total_train_loss) / (i+1)
        val_err[epoch], val_loss[epoch] = evaluate(net, val_loader, criterion)
        print(("Epoch {}: Train err: {}, Train loss: {} |"+
               "Validation err: {}, Validation loss: {}").format(
                   epoch + 1,
                   train_err[epoch],
                   train_loss[epoch],
                   val_err[epoch],
                   val_loss[epoch]))
        # Save the current model (checkpoint) to a file
        model_path = get_model_name(net.name, batch_size, learning_rate, epoch)
        torch.save(net.state_dict(), model_path)
    print('Finished Training')
    end_time = time.time()
    elapsed_time = end_time - start_time
    print("Total time elapsed: {:.2f} seconds".format(elapsed_time))
    # Write the train/test loss/err into CSV file for plotting later
    epochs = np.arange(1, num_epochs + 1)
    np.savetxt("{}_train_err.csv".format(model_path), train_err)
    np.savetxt("{}_train_loss.csv".format(model_path), train_loss)
    np.savetxt("{}_val_err.csv".format(model_path), val_err)
    np.savetxt("{}_val_loss.csv".format(model_path), val_loss)

"""### Part (b) -- 1pt

The parameters to the function `train_net` are hyperparameters of our neural network.
We made these hyperparameters easy to modify so that we can tune them later on.

What are the default values of the parameters `batch_size`, `learning_rate`,
and `num_epochs`?

The default values of the parameters in the train_net function are:

batch_size: 64
learning_rate: 0.01
num_epochs: 30

### Part (c) -- 3 pt

What files are written to disk when we call `train_net` with `small_net`, and train for 5 epochs? Provide a list
of all the files written to disk, and what information the files contain.

When I call train_net with small_net and train for 5 epochs, the following files are written to disk:

model_small_bs64_lr0.01_epoch0: This file contains the state dictionary of small_net after the first epoch. It holds all the learned parameters (weights and biases) of the model.

model_small_bs64_lr0.01_epoch1: State dictionary after the second epoch.

model_small_bs64_lr0.01_epoch2: State dictionary after the third epoch.

model_small_bs64_lr0.01_epoch3: State dictionary after the fourth epoch.

model_small_bs64_lr0.01_epoch4: State dictionary after the fifth epoch.

Additionally, four CSV files will be generated to store the training and validation errors and losses:

model_small_bs64_lr0.01_epoch4_train_err.csv: Contains the training errors for each epoch.

model_small_bs64_lr0.01_epoch4_train_loss.csv: Contains the training losses for each epoch.

model_small_bs64_lr0.01_epoch4_val_err.csv: Contains the validation errors for each epoch.

model_small_bs64_lr0.01_epoch4_val_loss.csv: Contains the validation losses for each epoch.

### Part (d) -- 2pt

Train both `small_net` and `large_net` using the function `train_net` and its default parameters.
The function will write many files to disk, including a model checkpoint (saved values of model weights)
at the end of each epoch.

If you are using Google Colab, you will need to mount Google Drive
so that the files generated by `train_net` gets saved. We will be using
these files in part (d).
(See the Google Colab tutorial for more information about this.)

Report the total time elapsed when training each network. Which network took longer to train?
Why?
"""

# Since the function writes files to disk, you will need to mount
# your Google Drive. If you are working on the lab locally, you
# can comment out this code.

from google.colab import drive
drive.mount('/content/gdrive')

train_net(small_net)
train_net(large_net)

"""small_net training is 147.37 secs

large_net training is 169.99 secs

large_net takes longer to train because it has more parameters and layers, requiring more computations during each forward and backward pass.

### Part (e) - 2pt

Use the function `plot_training_curve` to display the trajectory of the
training/validation error and the training/validation loss.
You will need to use the function `get_model_name` to generate the
argument to the `plot_training_curve` function.

Do this for both the small network and the large network. Include both plots
in your writeup.
"""

#model_path = get_model_name("small", batch_size=??, learning_rate=??, epoch=29)
# For small_net
model_path_small = get_model_name("small", batch_size=64, learning_rate=0.01, epoch=29)
plot_training_curve(model_path_small)

# For large_net
model_path_large = get_model_name("large", batch_size=64, learning_rate=0.01, epoch=29)
plot_training_curve(model_path_large)

"""### Part (f) - 5pt

Describe what you notice about the training curve.
How do the curves differ for `small_net` and `large_net`?
Identify any occurences of underfitting and overfitting.

For the larger network, the training error starts at 0.49 and decreases to 0.169 by epoch 30. The validation error initially decreases but starts to fluctuate later, indicating a possibility of overfitting. It took 169.99 seconds to train.

For the smaller network, the training error starts at 0.425375 and goes down to 0.269 by epoch 30. The validation error also shows a general decrease but has fluctuations, suggesting less overfitting compared to the larger network. It took 147.37 seconds to train.

The larger network took longer to train, likely due to having more parameters to update during each epoch. Both networks show signs of overfitting, but it's more pronounced in the larger network, as indicated by higher fluctuations in validation error.

## Part 3. Optimization Parameters [12 pt]

For this section, we will work with `large_net` only.

### Part (a) - 3pt

Train `large_net` with all default parameters, except set `learning_rate=0.001`.
Does the model take longer/shorter to train?
Plot the training curve. Describe the effect of *lowering* the learning rate.
"""

# Note: When we re-construct the model, we start the training
# with *random weights*. If we omit this code, the values of
# the weights will still be the previously trained values.
large_net = LargeNet()
train_net(large_net, learning_rate = 0.001)
large_model_path = get_model_name("large", batch_size=64, learning_rate=0.001, epoch=29)
plot_training_curve(large_model_path)

"""Lowering the learning rate to 0.001 results in slower convergence, as evidenced by the higher training and validation errors compared to the default learning rate. The model does show a decreasing trend in error and loss but at a much slower pace. The total time elapsed for training is 161.99 seconds, which is slightly less than with the default learning rate. Lowering the learning rate has made the learning process more cautious, potentially reducing the risk of overshooting the optimal values but requiring more time for the network to learn effectively.

### Part (b) - 3pt

Train `large_net` with all default parameters, except set `learning_rate=0.1`.
Does the model take longer/shorter to train?
Plot the training curve. Describe the effect of *increasing* the learning rate.
"""

large_net = LargeNet()
train_net(large_net, learning_rate = 0.1)
large_model_path = get_model_name("large", batch_size=64, learning_rate=0.1, epoch=29)
plot_training_curve(large_model_path)

"""With a learning rate of 0.1, the model showed erratic behavior in both training and validation errors/losses, suggesting that it might be overshooting the optimal values. The total time elapsed for training was 164.76 seconds, which is not substantially different from other settings. However, the training and validation errors are relatively unstable, indicating that the learning rate might be too high for the model to effectively learn the underlying pattern.

### Part (c) - 3pt

Train `large_net` with all default parameters, including with `learning_rate=0.01`.
Now, set `batch_size=512`. Does the model take longer/shorter to train?
Plot the training curve. Describe the effect of *increasing* the batch size.
"""

large_net = LargeNet()
train_net(large_net, batch_size = 512)
large_model_path = get_model_name("large", batch_size=512, learning_rate=0.01, epoch=29)
plot_training_curve(large_model_path)

"""With a batch size of 512, the model took 146.03 seconds to train, which is shorter compared to previous settings with smaller batch sizes. This is expected as larger batch sizes usually result in fewer iterations per epoch, thus speeding up the training process. However, the training and validation errors remained relatively high. The model's slower convergence might be due to the larger batch size, which provides a more accurate estimate of the gradient but also requires more data to make significant updates. In summary, increasing the batch size made the training faster but might have slowed down the model's convergence to an optimal solution.

### Part (d) - 3pt

Train `large_net` with all default parameters, including with `learning_rate=0.01`.
Now, set `batch_size=16`. Does the model take longer/shorter to train?
Plot the training curve. Describe the effect of *decreasing* the batch size.
"""

large_net = LargeNet()
train_net(large_net, batch_size = 16)
large_model_path = get_model_name("large", batch_size=16, learning_rate=0.01, epoch=29)
plot_training_curve(large_model_path)

"""With a batch size of 16, the model took 243.36 seconds to train, which is longer than the previous settings with larger batch sizes. The smaller batch size leads to more frequent updates, thus increasing the computational time. The training error and training loss decreased substantially, suggesting better fitting to the training data. However, the validation error did not show a corresponding decrease, and even increased towards the end, indicating overfitting. This suggests that smaller batch sizes may provide a less stable gradient estimate, leading to overfitting. Overall, decreasing the batch size made the model take longer to train and increased the risk of overfitting.

## Part 4. Hyperparameter Search [6 pt]

### Part (a) - 2pt

Based on the plots from above, choose another set of values for the hyperparameters (network, batch_size, learning_rate)
that you think would help you improve the validation accuracy. Justify your choice.

Network: large_net - The larger network has more capacity and generally performs better than the smaller one, given that we tune other hyperparameters well to avoid overfitting.

Batch Size: 512 - A larger batch size seemed to make the model train a bit faster without compromising much on performance. It also helps in stabilizing the gradient estimate.

Learning Rate: 0.01 - The default learning rate seems to provide a good balance. Lowering it made the model train very slowly, and increasing it led to instability.

These choices aim to balance model capacity, training stability, and speed. The large_net should capture complex features, the larger batch size should make training more stable and faster, and the default learning rate should be sufficient for effective learning.

### Part (b) - 1pt

Train the model with the hyperparameters you chose in part(a), and include the training curve.
"""

large_net = LargeNet()
train_net(large_net, learning_rate = 0.01, batch_size = 512, num_epochs = 30)
large_model_path = get_model_name("large", batch_size=512, learning_rate=0.01, epoch=29)
plot_training_curve(large_model_path)

"""### Part (c) - 2pt

Based on your result from Part(a), suggest another set of hyperparameter values to try.
Justify your choice.

Network: large_net - this network generally provides more capacity for learning complex patterns compared to small_net.

Learning Rate: 0.005 - A lower learning rate can make the model converge more smoothly and possibly avoid overshooting, especially if the model is sensitive to weight updates.

Batch Size: 64 - It's a balance between the computational efficiency offered by larger batch sizes and the more frequent model updates offered by smaller ones.

By opting for a lower learning rate, I am cautious about not updating the weights too aggressively, which could help with generalization. The moderate batch size of 64 allows for efficient training while still offering a good approximation of the gradient.

### Part (d) - 1pt

Train the model with the hyperparameters you chose in part(c), and include the training curve.
"""

large_net = LargeNet()
train_net(large_net, learning_rate = 0.005, batch_size = 64, num_epochs = 30)
large_model_path = get_model_name("large", batch_size= 64, learning_rate= 0.005, epoch=29)
plot_training_curve(large_model_path)

"""## Part 4. Evaluating the Best Model [15 pt]

### Part (a) - 1pt

Choose the **best** model that you have so far. This means choosing the best model checkpoint,
including the choice of `small_net` vs `large_net`, the `batch_size`, `learning_rate`,
**and the epoch number**.

Modify the code below to load your chosen set of weights to the model object `net`.
"""

net = LargeNet()
model_path = get_model_name(net.name, batch_size=64, learning_rate=0.005, epoch=29)
state = torch.load(model_path)
net.load_state_dict(state)

"""### Part (b) - 2pt

Justify your choice of model from part (a).

Based on the results, the best model appears to be the one with the following parameters:

Network: large_net

Batch size: 64

Learning rate: 0.005

Epoch where validation error is lowest: Epoch 27 with Validation error of 0.2955

This model demonstrates a good balance of training and validation error, suggesting it generalizes well to new data. The training time is also reasonable, making it a good choice for further evaluations or deployments.

### Part (c) - 2pt

Using the code in Part 0, any code from lecture notes, or any code that you write,
compute and report the **test classification error** for your chosen model.
"""

# If you use the `evaluate` function provided in part 0, you will need to
# set batch_size > 1
train_loader, val_loader, test_loader, classes = get_data_loader(
    target_classes=["cat", "dog"],
    batch_size=64)

# Criterion
criterion = nn.BCEWithLogitsLoss()

# Evaluate
test_err, test_loss = evaluate(net, test_loader, criterion)

print(f"Test Error: {test_err}, Test Loss: {test_loss}")

"""### Part (d) - 3pt

How does the test classification error compare with the **validation error**?
Explain why you would expect the test error to be *higher* than the validation error.

The test error (0.292) is slightly lower than the validation error (0.307), which is not the typical outcome. Generally, the test error is expected to be higher because the model is optimized using the validation set. The validation set's repeated use for tuning could lead the model to perform better on it than on an unseen test set. In this case, the lower test error could be due to random variability or the test set being easier to generalize to.

### Part (e) - 2pt

Why did we only use the test data set at the very end?
Why is it important that we use the test data as little as possible?

The test dataset is reserved for the final evaluation to get an unbiased measure of the model's performance on unseen data. Using the test set during the model development process can lead to "data leakage," where the model becomes tailored to the test set, giving an overly optimistic estimate of its performance. This defeats the purpose of having an independent test set and can result in poor generalization to new, unseen data. Therefore, it's crucial to use the test data as little as possible to maintain its role as a true indicator of the model's ability to generalize.

### Part (f) - 5pt

How does the your best CNN model compare with an 2-layer ANN model (no convolutional layers) on classifying cat and dog images. You can use a 2-layer ANN architecture similar to what you used in Lab 1. You should explore different hyperparameter settings to determine how well you can do on the validation dataset. Once satisified with the performance, you may test it out on the test data.

Hint: The ANN in lab 1 was applied on greyscale images. The cat and dog images are colour (RGB) and so you will need to flatted and concatinate all three colour layers before feeding them into an ANN.
"""

class ANN(nn.Module):
    def __init__(self):
        super(ANN, self).__init__()
        self.name = "ANN"
        self.layer1 = nn.Linear(32 * 32 * 3, 32)
        self.layer2 = nn.Linear(32, 1)

    def forward(self, x):
        x = x.view(-1, 32 * 32 * 3)
        x = F.relu(self.layer1(x))
        x = self.layer2(x)
        x = x.squeeze(1)
        return x

ann_net = ANN()
train_net(ann_net, learning_rate = 0.005, batch_size = 64, num_epochs = 30)
ann_model_path = get_model_name("ANN", batch_size= 64, learning_rate= 0.005, epoch=29)
plot_training_curve(ann_model_path)

train_loader, val_loader, test_loader, classes = get_data_loader(
    target_classes=["cat", "dog"],
    batch_size=64)

# Criterion
criterion = nn.BCEWithLogitsLoss()

# Evaluate
test_err, test_loss = evaluate(ann_net, test_loader, criterion)

print(f"Test Error: {test_err}, Test Loss: {test_loss}")

"""The ANN model has a test error of 0.3785, which is higher than the test error of 0.292 for the best CNN model. This shows that the CNN model is more effective at classifying cat and dog images. The ANN model's test error is slightly higher than its lowest validation error, suggesting some overfitting. Overall, the CNN model outperforms the ANN model on this task."""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html /content/Lab2_Cats_vs_Dogs.ipynb