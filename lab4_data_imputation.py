# -*- coding: utf-8 -*-
"""Lab4 Data Imputation.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1B15uzEbRHhz3_OXeq_6NZSlfCU9iWpCl

# Lab 4: Data Imputation using an Autoencoder


In this lab, you will build and train an autoencoder to impute (or "fill in") missing data.

We will be using the
Adult Data Set provided by the UCI Machine Learning Repository [1], available
at https://archive.ics.uci.edu/ml/datasets/adult.
The data set contains census record files of adults, including their
age, martial status, the type of work they do, and other features.

Normally, people use this data set to build a supervised classification
model to classify whether a person is a high income earner.
We will not use the dataset for this original intended purpose.

Instead, we will perform the task of imputing (or "filling in") missing values in the dataset. For example,
we may be missing one person's martial status, and another person's age, and
a third person's level of education. Our model will predict the missing features
based on the information that we do have about each person.

We will use a variation of a denoising autoencoder to solve this data imputation
problem. Our autoencoder will be trained using inputs that have one categorical feature artificially
removed, and the goal of the autoencoder is to correctly reconstruct all features,
including the one removed from the input.

In the process, you are expected to learn to:

1. Clean and process continuous and categorical data for machine learning.
2. Implement an autoencoder that takes continuous and categorical (one-hot) inputs.
3. Tune the hyperparameters of an autoencoder.
4. Use baseline models to help interpret model performance.

[1] Dua, D. and Karra Taniskidou, E. (2017). UCI Machine Learning Repository [http://archive.ics.uci.edu/ml]. Irvine, CA: University of California, School of Information and Computer Science.


### What to submit

Submit a PDF file containing all your code, outputs, and write-up. You can produce a PDF of your Google Colab file by going to File > Print and then save as PDF. The Colab instructions have more information.

Do not submit any other files produced by your code.

Include a link to your colab file in your submission.

## Colab Link

Include a link to your Colab file here. If you would like the TA to look at your
Colab file in case your solutions are cut off, **please make sure that your Colab
file is publicly accessible at the time of submission**.

Colab Link: https://colab.research.google.com/drive/1B15uzEbRHhz3_OXeq_6NZSlfCU9iWpCl?usp=sharing
"""

import csv
import numpy as np
import random
import torch
from torch.utils.data import DataLoader, TensorDataset
import matplotlib.pyplot as plt

"""## Part 0

We will be using a package called `pandas` for this assignment.

If you are using Colab, `pandas` should already be available.
If you are using your own computer,
installation instructions for `pandas` are available here:
https://pandas.pydata.org/pandas-docs/stable/install.html
"""

import pandas as pd

"""# Part 1. Data Cleaning [15 pt]

The adult.data file is available at `https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data`

The function `pd.read_csv` loads the adult.data file into a pandas dataframe.
You can read about the pandas documentation for `pd.read_csv` at
https://pandas.pydata.org/pandas-docs/stable/reference/api/pandas.read_csv.html
"""

header = ['age', 'work', 'fnlwgt', 'edu', 'yredu', 'marriage', 'occupation',
 'relationship', 'race', 'sex', 'capgain', 'caploss', 'workhr', 'country']
df = pd.read_csv(
    "https://archive.ics.uci.edu/ml/machine-learning-databases/adult/adult.data",
    names=header,
    index_col=False)

df.shape # there are 32561 rows (records) in the data frame, and 14 columns (features)

"""### Part (a) Continuous Features [3 pt]

For each of the columns `["age", "yredu", "capgain", "caploss", "workhr"]`, report the minimum, maximum, and average value across the dataset.

Then, normalize each of the features `["age", "yredu", "capgain", "caploss", "workhr"]`
so that their values are always between 0 and 1.
Make sure that you are actually modifying the dataframe `df`.

Like numpy arrays and torch tensors,
pandas data frames can be sliced. For example, we can
display the first 3 rows of the data frame (3 records) below.
"""

df[:3] # show the first 3 records

"""Alternatively, we can slice based on column names,
for example `df["race"]`, `df["hr"]`, or even index multiple columns
like below.
"""

subdf = df[["age", "yredu", "capgain", "caploss", "workhr"]]
subdf[:3] # show the first 3 records

"""Numpy works nicely with pandas, like below:"""

np.sum(subdf["caploss"])

"""Just like numpy arrays, you can modify
entire columns of data rather than one scalar element at a time.
For example, the code  

`df["age"] = df["age"] + 1`

would increment everyone's age by 1.
"""

# Calculate minimum, maximum, and average values for the specified columns
min_values = subdf.min()
max_values = subdf.max()
mean_values = subdf.mean()

# Create a DataFrame to hold the calculated statistics for easy reporting
statistics = pd.DataFrame\
 ({'Minimum': min_values, 'Maximum': max_values,'Average': mean_values})

# Normalize the continuous features to be between 0 and 1
df[["age", "yredu", "capgain", "caploss", "workhr"]] = \
 (subdf - min_values) / (max_values - min_values)

# Format the statistics and the first 5 entries of the modified dataframe for display
formatted_statistics = statistics.transpose()
formatted_normalized_data =  \
df[["age", "yredu", "capgain", "caploss", "workhr"]].head()

# Print the formatted statistics and normalized data
print(formatted_statistics)
print(formatted_normalized_data)

"""### Part (b) Categorical Features [1 pt]

What percentage of people in our data set are male? Note that the data labels all have an unfortunate space in the beginning, e.g. " Male" instead of "Male".

What percentage of people in our data set are female?
"""

# hint: you can do something like this in pandas
sum(df["sex"] == " Male")

# Trim the whitespace from the "sex" column for accurate comparison
df['sex'] = df['sex'].str.strip()

# Calculate the counts
male_count = sum(df["sex"] == "Male")
female_count = sum(df["sex"] == "Female")

# Calculate the percentages
total_count = len(df)
male_percentage = (male_count / total_count) * 100
female_percentage = (female_count / total_count) * 100

# Print out the percentages with the percent symbol
print(f"Percentage of males: {male_percentage:.2f}%")
print(f"Percentage of females: {female_percentage:.2f}%")

"""### Part (c) [2 pt]

Before proceeding, we will modify our data frame in a couple more ways:

1. We will restrict ourselves to using a subset of the features (to simplify our autoencoder)
2. We will remove any records (rows) already containing missing values, and store them in a second dataframe. We will only use records without missing values to train our autoencoder.

Both of these steps are done for you, below.

How many records contained missing features? What percentage of records were removed?
"""

contcols = ["age", "yredu", "capgain", "caploss", "workhr"]
catcols = ["work", "marriage", "occupation", "edu", "relationship", "sex"]
features = contcols + catcols
df = df[features]

missing = pd.concat([df[c] == " ?" for c in catcols], axis=1).any(axis=1)
df_with_missing = df[missing]
df_not_missing = df[~missing]

# Calculate the number of records with missing features
num_records_with_missing = df_with_missing.shape[0]

# Calculate the total number of records before removing anything
total_records = df.shape[0]

# Calculate the percentage of records removed
percentage_removed = (num_records_with_missing / total_records) * 100

# Output the results
print(f"Number of records with missing features: {num_records_with_missing}")
print(f"Percentage of records removed: {percentage_removed:.2f}%")

"""### Part (d) One-Hot Encoding [1 pt]

What are all the possible values of the feature "work" in `df_not_missing`? You may find the Python function `set` useful.
"""

print(set(df_not_missing["work"]))

"""We will be using a one-hot encoding to represent each of the categorical variables.
Our autoencoder will be trained using these one-hot encodings.

We will use the pandas function `get_dummies` to produce one-hot encodings
for all of the categorical variables in `df_not_missing`.
"""

data = pd.get_dummies(df_not_missing)

data[:3]

"""### Part (e) One-Hot Encoding [2 pt]

The dataframe `data` contains the cleaned and normalized data that we will use to train our denoising autoencoder.

How many **columns** (features) are in the dataframe `data`?

Briefly explain where that number come from.
"""

print(len(data.columns))

"""The pd.get_dummies() function in pandas transforms each categorical feature into multiple binary features, with each new feature corresponding to one of the possible values of the original feature. The new features indicate the presence (with a 1) or absence (with a 0) of each possible value in the original data. This process is known as one-hot encoding.

For example, if the original work column in your dataset has 7 unique job types, get_dummies() will create 7 new columns, one for each job type (e.g., work_Federal-gov, work_Local-gov, and so on). For each record, only one of these new columns will have a 1, corresponding to the job type that record had in the original work column. All other new columns for that record will have a 0.

### Part (f) One-Hot Conversion [3 pt]

We will convert the pandas data frame `data` into numpy, so that
it can be further converted into a PyTorch tensor.
However, in doing so, we lose the column label information that
a panda data frame automatically stores.

Complete the function `get_categorical_value` that will return
the named value of a feature given a one-hot embedding.
You may find the global variables `cat_index` and `cat_values`
useful. (Display them and figure out what they are first.)

We will need this function in the next part of the lab
to interpret our autoencoder outputs. So, the input
to our function `get_categorical_values` might not
actually be "one-hot" -- the input may instead
contain real-valued predictions from our neural network.
"""

datanp = data.values.astype(np.float32)

cat_index = {}  # Mapping of feature -> start index of feature in a record
cat_values = {} # Mapping of feature -> list of categorical values the feature can take

# build up the cat_index and cat_values dictionary
for i, header in enumerate(data.keys()):
    if "_" in header: # categorical header
        feature, value = header.rsplit("_", 1)# remove the last char; it is always an underscore
        if feature not in cat_index:
            cat_index[feature] = i
            cat_values[feature] = [value]
        else:
            cat_values[feature].append(value)
print(cat_index)
print(cat_values)

def get_onehot(record, feature):
    """
    Return the portion of `record` that is the one-hot encoding
    of `feature`. For example, since the feature "work" is stored
    in the indices [5:12] in each record, calling `get_range(record, "work")`
    is equivalent to accessing `record[5:12]`.

    Args:
        - record: a numpy array representing one record, formatted
                  the same way as a row in `data.np`
        - feature: a string, should be an element of `catcols`
    """
    start_index = cat_index[feature]
    stop_index = cat_index[feature] + len(cat_values[feature])
    return record[start_index:stop_index]

def get_categorical_value(onehot, feature):
    """
    Return the categorical value name of a feature given
    a one-hot vector representing the feature.

    Args:
        - onehot: a numpy array one-hot representation of the feature
        - feature: a string, should be an element of `catcols`

    Examples:

    >>> get_categorical_value(np.array([0., 0., 0., 0., 0., 1., 0.]), "work")
    'State-gov'
    >>> get_categorical_value(np.array([0.1, 0., 1.1, 0.2, 0., 1., 0.]), "work")
    'Private'
    """
    # <----- TODO: WRITE YOUR CODE HERE ----->
    # You may find the variables `cat_index` and `cat_values`
    # (created above) useful.
    ind = np.argmax(onehot)
    values = cat_values[feature]
    return values[ind]

# more useful code, used during training, that depends on the function
# you write above

def get_feature(record, feature):
    """
    Return the categorical feature value of a record
    """
    onehot = get_onehot(record, feature)
    return get_categorical_value(onehot, feature)

def get_features(record):
    """
    Return a dictionary of all categorical feature values of a record
    """
    return { f: get_feature(record, f) for f in catcols }

"""### Part (g) Train/Test Split [3 pt]

Randomly split the data into approximately 70% training, 15% validation and 15% test.

Report the number of items in your training, validation, and test set.
"""

# set the numpy seed for reproducibility
# https://docs.scipy.org/doc/numpy/reference/generated/numpy.random.seed.html

np.random.seed(50)  # For reproducibility
indices = np.arange(datanp.shape[0])
np.random.shuffle(indices)

# Define split index
train_index = int(0.7 * datanp.shape[0])
valid_index = int(0.85 * datanp.shape[0])

# Split data
train_data = datanp[:train_index]
valid_data = datanp[train_index:valid_index]
test_data = datanp[valid_index:]

# Report the number of items in each set
num_train = len(train_data)
num_validation = len(valid_data)
num_test = len(test_data)

print(f'Number of items in training set: {num_train}')
print(f'Number of items in validation set: {num_validation}')
print(f'Number of items in test set: {num_test}')

"""## Part 2. Model Setup [5 pt]

### Part (a) [4 pt]

Design a fully-connected autoencoder by modifying the `encoder` and `decoder`
below.

The input to this autoencoder will be the features of the `data`, with
one categorical feature recorded as "missing". The output of the autoencoder
should be the reconstruction of the same features, but with the missing
value filled in.

**Note**: Do not reduce the dimensionality of the input too much!
The output of your embedding is expected to contain information
about ~11 features.
"""

from torch import nn

class AutoEncoder(nn.Module):
    def __init__(self):
        self.name = "AutoEncoder"
        super(AutoEncoder, self).__init__()
        self.encoder = nn.Sequential(
            nn.Linear(57, 50),  # Compress input features from 57 to 50
            nn.ReLU(),          # Apply ReLU activation function
            nn.Linear(50, 32),  # Further compress features from 50 to 32
            nn.ReLU(),          # Apply ReLU activation function
            nn.Linear(32, 16)   # Compress down to an encoding of size 16
        )
        self.decoder = nn.Sequential(
            nn.Linear(16, 32),  # Decompress features from 16 to 32
            nn.ReLU(),          # Apply ReLU activation function
            nn.Linear(32, 50),  # Decompress features from 32 to 50
            nn.ReLU(),          # Apply ReLU activation function
            nn.Linear(50, 57),  # Decompress features back to original size of 57
            nn.Sigmoid()        # Apply sigmoid to get outputs in the range (0, 1)
        )

    def forward(self, x):
        x = self.encoder(x)
        x = self.decoder(x)
        return x

"""### Part (b) [1 pt]

Explain why there is a sigmoid activation in the last step of the decoder.

(**Note**: the values inside the data frame `data` and the training code in Part 3 might be helpful.)

The sigmoid activation function in the last layer of the decoder is used because it constrains the output of the autoencoder to be within the range of 0 to 1. This is particularly important because the input data to the autoencoder has been normalized to lie within this range, which is a common preprocessing step.

## Part 3. Training [18]

### Part (a) [6 pt]

We will train our autoencoder in the following way:

- In each iteration, we will hide one of the categorical features using the `zero_out_random_features` function
- We will pass the data with one missing feature through the autoencoder, and obtain a reconstruction
- We will check how close the reconstruction is compared to the original data -- including the value of the missing feature

Complete the code to train the autoencoder, and plot the training and validation loss every few iterations.
You may also want to plot training and validation "accuracy" every few iterations, as we will define in
part (b). You may also want to checkpoint your model every few iterations or epochs.

Use `nn.MSELoss()` as your loss function. (Side note: you might recognize that this loss function is not
ideal for this problem, but we will use it anyway.)
"""

def zero_out_feature(records, feature):
    """ Set the feature missing in records, by setting the appropriate
    columns of records to 0
    """
    start_index = cat_index[feature]
    stop_index = cat_index[feature] + len(cat_values[feature])
    records[:, start_index:stop_index] = 0
    return records

def zero_out_random_feature(records):
    """ Set one random feature missing in records, by setting the
    appropriate columns of records to 0
    """
    return zero_out_feature(records, random.choice(catcols))

def train(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-4):
    """ Training loop. You should update this."""
    torch.manual_seed(42)
    criterion = nn.MSELoss()
    optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)

    # For plotting
    iters, train_losses, valid_losses, train_acc, valid_acc = [], [], [], [], []

    for epoch in range(num_epochs):
        model.train()  # Training mode
        total_train_loss = 0
        for data in train_loader:
            datam = zero_out_random_feature(data.clone()) # zero out one categorical feature
            recon = model(datam)
            loss = criterion(recon, data)
            loss.backward()
            optimizer.step()
            optimizer.zero_grad()
            total_train_loss += loss.item() * data.size(0)  # Multiply by batch size

        avg_train_loss = total_train_loss / len(train_loader.dataset)  # Compute average loss
        train_losses.append(avg_train_loss)
        train_acc.append(get_accuracy(model, train_loader))

        # Validation
        model.eval()  # Evaluation mode
        total_valid_loss = 0
        with torch.no_grad():
            for data in valid_loader:
                datam = zero_out_random_feature(data.clone())  # zero out one categorical feature
                recon = model(datam)
                loss = criterion(recon, data)
                total_valid_loss += loss.item() * data.size(0)  # Multiply by batch size

        avg_valid_loss = total_valid_loss / len(valid_loader.dataset)  # Compute average loss
        valid_losses.append(avg_valid_loss)
        valid_acc.append(get_accuracy(model, valid_loader))  # Assuming get_accuracy is defined

        iters.append(epoch)
        print(f"Epoch {epoch+1}, Train Loss: {avg_train_loss:.4f}, \
        Validation Loss: {avg_valid_loss:.4f},\
         Train Acc: {train_acc[-1]:.4f}, Validation Acc: {valid_acc[-1]:.4f}")

        # Save model checkpoint
        batch_size = train_loader.batch_size
        model_path = \
        f"model_{model.name}_bs{batch_size}_lr{learning_rate}_epoch{epoch}.pth"
        torch.save(model.state_dict(), model_path)

    # Plotting
    plt.figure(figsize=(10, 5))
    plt.title("Training Curve - Loss")
    plt.plot(iters, train_losses, label="Train")
    plt.plot(iters, valid_losses, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Loss")
    plt.legend()
    plt.show()

    plt.figure(figsize=(10, 5))
    plt.title("Training Curve - Accuracy")
    plt.plot(iters, train_acc, label="Train")
    plt.plot(iters, valid_acc, label="Validation")
    plt.xlabel("Epoch")
    plt.ylabel("Accuracy")
    plt.legend()
    plt.show()

    print(f"Final Training Accuracy: {train_acc[-1]:.4f}")
    print(f"Final Validation Accuracy: {valid_acc[-1]:.4f}")

"""### Part (b) [3 pt]

While plotting training and validation loss is valuable, loss values are harder to compare
than accuracy percentages. It would be nice to have a measure of "accuracy" in this problem.

Since we will only be imputing missing categorical values, we will define an accuracy measure.
For each record and for each categorical feature, we determine whether
the model can predict the categorical feature given all the other features of the record.

A function `get_accuracy` is written for you. It is up to you to figure out how to
use the function. **You don't need to submit anything in this part.**
To earn the marks, correctly plot the training and validation accuracy every few
iterations as part of your training curve.
"""

def get_accuracy(model, data_loader):
    """Return the "accuracy" of the autoencoder model across a data set.
    That is, for each record and for each categorical feature,
    we determine whether the model can successfully predict the value
    of the categorical feature given all the other features of the
    record. The returned "accuracy" measure is the percentage of times
    that our model is successful.

    Args:
       - model: the autoencoder model, an instance of nn.Module
       - data_loader: an instance of torch.utils.data.DataLoader

    Example (to illustrate how get_accuracy is intended to be called.
             Depending on your variable naming this code might require
             modification.)

        >>> model = AutoEncoder()
        >>> vdl = torch.utils.data.DataLoader(data_valid, batch_size=256, shuffle=True)
        >>> get_accuracy(model, vdl)
    """
    total = 0
    acc = 0
    for col in catcols:
        for item in data_loader: # minibatches
            inp = item.detach().numpy()
            out = model(zero_out_feature(item.clone(), col)).detach().numpy()
            for i in range(out.shape[0]): # record in minibatch
                acc += int(get_feature(out[i], col) == get_feature(inp[i], col))
                total += 1
    return acc / total

"""### Part (c) [4 pt]

Run your updated training code, using reasonable initial hyperparameters.

Include your training curve in your submission.
"""

model = AutoEncoder()

train_loader = torch.utils.data.DataLoader(train_data, batch_size=64, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=64, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=64, shuffle=True)

train(model, train_loader, valid_loader, num_epochs=10, learning_rate=1e-4)

"""### Part (d) [5 pt]

Tune your hyperparameters, training at least 4 different models (4 sets of hyperparameters).

Do not include all your training curves. Instead, explain what hyperparameters
you tried, what their effect was, and what your thought process was as you
chose the next set of hyperparameters to try.

Model 1: Baseline Model

Parameters: Learning rate = 1e-4, Epochs = 10, Batch size = 64.

Final Accuracy: Training 56.59%, Validation 56.33%.

Observations: The model showed continuous improvement, with a notable plateau towards the end. The closely aligned training and validation accuracies indicated effective learning without overfitting.

Thought Process: The solid baseline performance suggested potential for improvement. The focus shifted to testing a higher learning rate to enhance learning efficiency.

Model 2: Increased Learning Rate

Parameters: Learning rate = 2e-4, Epochs = 10, Batch size = 64.

Final Accuracy: Training 58.90%, Validation 58.73%.

Observations: Accelerated learning was observed with slightly higher accuracy. The slight dip in later epochs suggested potential overfitting or learning instability.

Thought Process: The higher learning rate improved learning speed but raised concerns about stability. A moderately increased learning rate was considered for the next model.

Model 3: Reduced Learning Rate

Parameters: Learning rate = 1.5e-4, Epochs = 10, Batch size = 64.

Final Accuracy: Training 59.02%, Validation 58.54%.

Observations: The model achieved steady and consistent improvement with less fluctuation in accuracy.

Thought Process: The learning rate at 1.5e-4 provided a balance between speed and stability. The next step involved testing a longer training duration to explore further improvements.

Model 4: Increased Epochs

Parameters: Learning rate = 1.5e-4, Epochs = 20, Batch size = 64.

Final Accuracy: Training 59.46%, Validation 59.22%.

Observations: Extended training resulted in a lower overall loss, but a slight decline in accuracy towards the end suggested a limit to the benefits of additional epochs.

Thought Process: The model showed signs of plateauing or overfitting with extended training. A change in batch size was considered to explore the impact on model performance.

Model 5: Smaller Batch Size

Parameters: Learning rate = 1.5e-4, Epochs = 20, Batch size = 32.

Final Accuracy: Training 60.82%, Validation 60.66%.

Observations: The smaller batch size led to the highest accuracies, indicating more effective learning and generalization.

Thought Process: Smaller batches provided more nuanced updates, enhancing the model's learning capability. This setup achieved the best balance and yielded the highest performance, indicating its effectiveness for this specific task and dataset.
"""

best_model = AutoEncoder()

train_loader = torch.utils.data.DataLoader(train_data, batch_size=32, shuffle=True)
valid_loader = torch.utils.data.DataLoader(valid_data, batch_size=32, shuffle=True)
test_loader = torch.utils.data.DataLoader(test_data, batch_size=32, shuffle=True)

train(best_model, train_loader, valid_loader, num_epochs=20, learning_rate=1.5e-4)

"""## Part 4. Testing [12 pt]

### Part (a) [2 pt]

Compute and report the test accuracy.
"""

print(get_accuracy(best_model, test_loader))

"""### Part (b) [4 pt]

Based on the test accuracy alone, it is difficult to assess whether our model
is actually performing well. We don't know whether a high accuracy is due to
the simplicity of the problem, or if a poor accuracy is a result of the inherent
difficulty of the problem.

It is therefore very important to be able to compare our model to at least one
alternative. In particular, we consider a simple **baseline**
model that is not very computationally expensive. Our neural network
should at least outperform this baseline model. If our network is not much
better than the baseline, then it is not doing well.

For our data imputation problem, consider the following baseline model:
to predict a missing feature, the baseline model will look at the **most common value** of the feature in the training set.

For example, if the feature "marriage" is missing, then this model's prediction will be the most common value for "marriage" in the training set, which happens to be "Married-civ-spouse".

What would be the test accuracy of this baseline model?

"""

most_common = {}
for col in df_not_missing.columns:
  # get the most common value for each column
  most_common[col] = df_not_missing[col].value_counts().idxmax()

accuracy = sum(df_not_missing['marriage'] == \
               most_common['marriage'])/len(df_not_missing)
print("The accuracy for baseline model of missing 'marriage' test is:", accuracy)

"""### Part (c) [1 pt]

How does your test accuracy from part (a) compared to your basline test accuracy in part (b)?

The test accuracy in part(a) is much better than the accuracy in part (b). In part (a), we are making predictions based on the embedding features about all the information constructed by the autoencoders, and it should be better than simply using the most common value to predict missing feature.

### Part (d) [1 pt]

Look at the first item in your test data.
Do you think it is reasonable for a human
to be able to guess this person's education level
based on their other features? Explain.
"""

get_features(test_data[0])

"""Given the individual's occupation as 'Prof-specialty' in the private sector and their status as 'Never-married' and 'Not-in-family', it is somewhat reasonable for a human to guess that the person has a higher education level, such as a bachelor's degree. Occupations labeled as 'Prof-specialty' often require higher education, suggesting a likelihood of this individual having at least a bachelor's degree. However, this guess would be largely based on general occupational trends and might not accurately represent every individual's unique educational journey. Other provided features like 'Never-married', 'Not-in-family', and 'Female' don't offer direct insights into educational attainment, making the occupation the most telling clue in this context.

### Part (e) [2 pt]

What is your model's prediction of this person's education
level, given their other features?
"""

out = best_model(zero_out_feature(torch.tensor(test_data[0]).view(1,57), "edu")).detach().numpy()
print(get_feature(out[0],"edu"))
# the output of my model is Masters, it is not correct but it makes sense
# since the person has an occupation as a prof-speciality.

"""### Part (f) [2 pt]

What is the baseline model's prediction
of this person's education level?
"""

print(most_common['edu'])

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html "/content/Lab4_Data_Imputation.ipynb"