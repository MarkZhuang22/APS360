# -*- coding: utf-8 -*-
"""Lab5 Spam Detection.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1c4UATfigjBWI0J7EonHWsZRDwI9pZNOA

# Lab 5: Spam Detection

In this assignment, we will build a recurrent neural network to classify a SMS text message
as "spam" or "not spam". In the process, you will
    
1. Clean and process text data for machine learning.
2. Understand and implement a character-level recurrent neural network.
3. Use torchtext to build recurrent neural network models.
4. Understand batching for a recurrent neural network, and use torchtext to implement RNN batching.

### What to submit

Submit a PDF file containing all your code, outputs, and write-up. You can produce a PDF of your Google Colab file by going to File > Print and then save as PDF. The Colab instructions have more information.

Do not submit any other files produced by your code.

Include a link to your colab file in your submission.

## Colab Link

Include a link to your Colab file here. If you would like the TA to look at your
Colab file in case your solutions are cut off, **please make sure that your Colab
file is publicly accessible at the time of submission**.

Colab Link: https://colab.research.google.com/drive/1c4UATfigjBWI0J7EonHWsZRDwI9pZNOA?usp=sharing

As we are using the older version of the torchtext, please run the following to downgrade the torchtext version:

!pip install -U torch==1.8.0+cu111 torchtext==0.9.0 -f https://download.pytorch.org/whl/torch_stable.html

If you are interested to use the most recent version if torchtext, you can look at the following document to see how to convert the legacy version to the new version:
https://colab.research.google.com/github/pytorch/text/blob/master/examples/legacy_tutorial/migration_tutorial.ipynb
"""

!pip install -U torch==1.11.0+cu113 torchtext==0.6.0 -f https://download.pytorch.org/whl/torch_stable.html

!pip install -U torch==1.11.0 torchtext==0.6.0

# Reload environment
exit()

import torch
import torch.nn as nn
import torch.nn.functional as F
import torch.optim as optim
import numpy as np
import matplotlib.pyplot as plt
import torchtext
from torchtext.data import Field, TabularDataset, LabelField
import random

"""## Part 1. Data Cleaning [15 pt]

We will be using the "SMS Spam Collection Data Set" available at http://archive.ics.uci.edu/ml/datasets/SMS+Spam+Collection

There is a link to download the "Data Folder" at the very top of the webpage. Download the zip file, unzip it, and upload the file `SMSSpamCollection` to Colab.

### Part (a) [2 pt]

Open up the file in Python, and print out one example of a spam SMS, and one example of a non-spam SMS.

What is the label value for a spam message, and what is the label value for a non-spam message?
"""

# Open the file and read lines
with open('SMSSpamCollection', 'r') as file:
    lines = file.readlines()

# Initialize variables to store examples
spam_example = None
ham_example = None

# Iterate over lines to find examples
for line in lines:
    if spam_example is None and line.lower().startswith('spam'):
        spam_example = line
    elif ham_example is None and line.lower().startswith('ham'):
        ham_example = line

    # Break the loop if both examples are found
    if spam_example is not None and ham_example is not None:
        break

# Print the examples
print("Spam SMS Example:", spam_example)
print("Non-Spam (Ham) SMS Example:", ham_example)

"""### Part (b) [1 pt]

How many spam messages are there in the data set?
How many non-spam messages are there in the data set?

"""

# Initialize counters
spam_count = 0
ham_count = 0

# Read the file and count spam and ham messages
with open('SMSSpamCollection', 'r') as file:
    for line in file:
        if line.startswith('spam'):
            spam_count += 1
        elif line.startswith('ham'):
            ham_count += 1

# Print the counts
print("Number of spam messages:", spam_count)
print("Number of non-spam (ham) messages:", ham_count)

"""### Part (c) [4 pt]

We will be using the package `torchtext` to load, process, and batch the data.
A tutorial to torchtext is available below. This tutorial uses the same
Sentiment140 data set that we explored during lecture.

https://medium.com/@sonicboom8/sentiment-analysis-torchtext-55fb57b1fab8

Unlike what we did during lecture, we will be building a **character level RNN**.
That is, we will treat each **character** as a token in our sequence,
rather than each **word**.

Identify two advantage and two disadvantage of modelling SMS text
messages as a sequence of characters rather than a sequence of words.

Advantages of Character-Level RNN:

Handling of Rare and Unknown Words: Character-level models are not affected by rare or unknown words, which are common in SMS texts due to typos, slang, and abbreviations. Since the model works with characters, it doesn't need to have seen a word during training to be able to process it during inference.

Capturing Morphological and Subword Information: Character-level models can capture subword information, such as prefixes and suffixes, which can be especially important in languages with rich morphology. This can help in understanding the meaning of words that are not explicitly present in the training data.

Disadvantages of Character-Level RNN:

Longer Sequences: Characters result in longer sequences compared to words, leading to more computationally intensive processing and potentially more difficult learning tasks. RNNs might struggle with very long dependencies, which are common in character-level models.

Loss of Higher-Level Abstractions: Word-level models can capture higher-level abstractions and semantics more easily than character-level models. Characters do not inherently carry as much meaningful information as words, so the model may need more data and complexity to learn equivalent patterns and meanings.

### Part (d) [1 pt]

We will be loading our data set using `torchtext.data.TabularDataset`. The
constructor will read directly from the `SMSSpamCollection` file.

For the data file to be read successfuly, we
need to specify the **fields** (columns) in the file.
In our case, the dataset has two fields:

- a text field containing the sms messages,
- a label field which will be converted into a binary label.

Split the dataset into `train`, `valid`, and `test`. Use a 60-20-20 split.
You may find this torchtext API page helpful:
https://torchtext.readthedocs.io/en/latest/data.html#dataset

Hint: There is a `Dataset` method that can perform the random split for you.
"""

# Define the text field
text_field = torchtext.data.Field(sequential=True,      # text sequence
                                  tokenize=lambda x: x, # because we are building a character-RNN
                                  include_lengths=True, # to track the length of sequences, for batching
                                  batch_first=True,
                                  use_vocab=True)

# Define the label field
label_field = torchtext.data.Field(sequential=False,    # not a sequence
                                  use_vocab=False,     # don't need to track vocabulary
                                  is_target=True,
                                  batch_first=True,
                                  preprocessing=lambda x: int(x == 'spam')) # convert text to 0 and 1

fields = [('label', label_field), ('text', text_field)]

# Load the dataset
dataset = TabularDataset(
    path='SMSSpamCollection', # name of the file
    format='tsv',             # fields are separated by a tab
    fields=fields,
    skip_header=False
)

# Split the dataset into train, validation, and test sets
train, valid, test = dataset.split(split_ratio=[0.6, 0.2, 0.2], random_state = np.random.seed(54))

# Print some information about the datasets
print(f"Number of training examples: {len(train)}")
print(f"Number of validation examples: {len(valid)}")
print(f"Number of testing examples: {len(test)}")

"""### Part (e) [2 pt]

You saw in part (b) that there are many more non-spam messages than spam messages.
This **imbalance** in our training data will be problematic for training.
We can fix this disparity by duplicating spam messages in the training set,
so that the training set is roughly **balanced**.

Explain why having a balanced training set is helpful for training our neural network.

Note: if you are not sure, try removing the below code and train your mode.
"""

# save the original training examples
old_train_examples = train.examples
# get all the spam messages in `train`
train_spam = []
for item in train.examples:
    if item.label == 1:
        train_spam.append(item)
# duplicate each spam message 6 more times
train.examples = old_train_examples + train_spam * 6
print(len(train.examples))

"""Balancing a training set in tasks like spam detection is essential because it prevents model bias towards the majority class, ensures adequate learning from both classes, promotes better generalization to unseen data, and leads to more accurate and meaningful performance metrics. This balanced approach enables the neural network to develop a nuanced understanding of all classes, enhancing its overall predictive performance.

### Part (f) [1 pt]

We need to build the vocabulary on the training data by running the below code.
This finds all the possible character tokens in the training set.

Explain what the variables `text_field.vocab.stoi` and `text_field.vocab.itos` represent.
"""

text_field.build_vocab(train)
print(text_field.vocab.stoi)
print(text_field.vocab.itos)

"""The `text_field.vocab.stoi` is a dictionary mapping each token (character) to its numerical index in the vocabulary, while `text_field.vocab.itos` is a list where each index corresponds to the token (character) at that position, essentially mapping indices back to tokens.

### Part (g) [2 pt]

The tokens `<unk>` and `<pad>` were not in our SMS text messages.
What do these two values represent?

1. **`<unk>` (Unknown Token)**: This token represents any word or character that is not found in the training dataset's vocabulary. During training, if the model encounters a word or character in the validation or test set that it hasn't seen before (i.e., it's not in the training vocabulary), it will be represented as `<unk>`. This helps the model handle out-of-vocabulary (OOV) words or characters.

2. **`<pad>` (Padding Token)**: The `<pad>` token is used to fill in sequences to ensure they are all the same length when batching sequences together. Most models require input data of uniform size, but natural language data often varies in length. Padding shorter sequences with the `<pad>` token allows for consistent input size. During training and inference, these padding tokens are typically ignored or treated specially to ensure they don't affect the model's understanding of the actual content.

### Part (h) [2 pt]

Since text sequences are of variable length, `torchtext` provides a `BucketIterator` data loader,
which batches similar length sequences together. The iterator also provides functionalities to
pad sequences automatically.

Take a look at 10 batches in `train_iter`. What is the maximum length of the
input sequence in each batch? How many `<pad>` tokens are used in each of the 10
batches?
"""

train_iter = torchtext.data.BucketIterator(train,
                                           batch_size=32,
                                           sort_key=lambda x: len(x.text), # to minimize padding
                                           sort_within_batch=True,        # sort within each batch
                                           repeat=False)                  # repeat the iterator for many epochs

# Iterating over the first 10 batches
for i, batch in enumerate(train_iter):
    if i >= 10:
        break  # Stop after 10 batches

    # The first element is the text data and the second is its length
    sms_data = batch.text[0]
    batch_max_length = sms_data.shape[1]  # The second dimension is the length of sequences
    num_pad_tokens = (sms_data == text_field.vocab.stoi[text_field.pad_token]).sum().item()

    print(f"Batch {i + 1}:")
    print(f"Maximum length of input sequence: {batch_max_length}")
    print(f"Number of <pad> tokens: {num_pad_tokens}\n")

"""## Part 2. Model Building [8 pt]

Build a recurrent neural network model, using an architecture of your choosing.
Use the one-hot embedding of each character as input to your recurrent network.
Use one or more fully-connected layers to make the prediction based on your
recurrent network output.

Instead of using the RNN output value for the final token, another often used
strategy is to max-pool over the entire output array. That is, instead of calling
something like:

```
out, _ = self.rnn(x)
self.fc(out[:, -1, :])
```

where `self.rnn` is an `nn.RNN`, `nn.GRU`, or `nn.LSTM` module, and `self.fc` is a
fully-connected
layer, we use:

```
out, _ = self.rnn(x)
self.fc(torch.max(out, dim=1)[0])
```

This works reasonably in practice. An even better alternative is to concatenate the
max-pooling and average-pooling of the RNN outputs:

```
out, _ = self.rnn(x)
out = torch.cat([torch.max(out, dim=1)[0],
                 torch.mean(out, dim=1)], dim=1)
self.fc(out)
```

We encourage you to try out all these options. The way you pool the RNN outputs
is one of the "hyperparameters" that you can choose to tune later on.
"""

# You might find this code helpful for obtaining
# PyTorch one-hot vectors.

ident = torch.eye(10)
print(ident[0]) # one-hot vector
print(ident[1]) # one-hot vector
x = torch.tensor([[1, 2], [3, 4]])
print(ident[x]) # one-hot vectors

class RNN(nn.Module):
    def __init__(self, input_dim, hidden_dim, output_dim, num_layers):
        super(RNN, self).__init__()

        # Define the RNN layer
        self.rnn = nn.RNN(input_dim, hidden_dim, num_layers, batch_first=True)

        # Define the fully connected layer
        self.fc = nn.Linear(hidden_dim * 2, output_dim)

        # Define the identity matrix for one-hot encoding
        self.ident = torch.eye(input_dim)

    def forward(self, x):
        # Transform input to one-hot encoded vectors
        x_one_hot = self.ident[x]

        # Passing in the input and hidden state into the model and obtaining outputs
        out, _ = self.rnn(x_one_hot)

        # Pooling strategies
        max_pool = torch.max(out, dim=1)[0]
        avg_pool = torch.mean(out, dim=1)
        out = torch.cat((max_pool, avg_pool), dim=1)

        # Passing the pooled output to the fully connected layer
        out = self.fc(out)

        return out

"""## Part 3. Training [16 pt]

### Part (a) [4 pt]

Complete the `get_accuracy` function, which will compute the
accuracy (rate) of your model across a dataset (e.g. validation set).
You may modify `torchtext.data.BucketIterator` to make your computation
faster.
"""

def get_accuracy(model, data_loader):
    """ Compute the accuracy of the `model` across a dataset `data_loader` """
    # Ensure the model is in evaluation mode, which turns off dropout
    model.eval()

    correct, total = 0, 0
    with torch.no_grad():  # No need to track gradients for validation
        for batch in data_loader:
            inputs, labels = batch.text[0], batch.label

            # Forward pass
            outputs = model(inputs)

            # Get the predicted class with the highest score
            predicted = outputs.argmax(1)

            # Update totals
            correct += (predicted == labels).sum().item()
            total += labels.shape[0]

    # Calculate accuracy
    accuracy = correct / total
    return accuracy

"""### Part (b) [4 pt]

Train your model. Plot the training curve of your final model.
Your training curve should have the training/validation loss and
accuracy plotted periodically.

Note: Not all of your batches will have the same batch size.
In particular, if your training set does not divide evenly by
your batch size, there will be a batch that is smaller than
the rest.
"""

def train_rnn_network(model, train_loader, valid_loader, num_epochs=5, learning_rate=1e-5):
    criterion = nn.CrossEntropyLoss()
    optimizer = optim.Adam(model.parameters(), lr=learning_rate)
    train_losses, valid_losses = [], []
    train_acc, valid_acc = [], []

    for epoch in range(num_epochs):
        model.train()  # Set the model to training mode
        total_train_loss = 0
        total_train_samples = 0

        for batch in train_loader:
            inputs, labels = batch.text[0], batch.label
            optimizer.zero_grad()
            pred = model(inputs)
            loss = criterion(pred, labels)
            loss.backward()
            optimizer.step()
            total_train_loss += loss.item()
            total_train_samples += labels.shape[0]

        avg_train_loss = total_train_loss / total_train_samples
        train_losses.append(avg_train_loss)
        train_acc.append(get_accuracy(model, train_loader))

        model.eval()  # Set the model to evaluation mode
        total_valid_loss = 0
        total_valid_samples = 0

        with torch.no_grad():
            for batch in valid_loader:
                inputs, labels = batch.text[0], batch.label
                pred = model(inputs)
                loss = criterion(pred, labels)
                total_valid_loss += loss.item()
                total_valid_samples += labels.shape[0]

        avg_valid_loss = total_valid_loss / total_valid_samples
        valid_losses.append(avg_valid_loss)
        valid_acc.append(get_accuracy(model, valid_loader))

        print(f'Epoch [{epoch+1}/{num_epochs}], \
        Train Loss:{avg_train_loss:.4f}, Train Acc: {train_acc[-1]:.4f}, \
        Valid Loss:{avg_valid_loss:.4f}, Valid Acc: {valid_acc[-1]:.4f}')

    # Plotting training and validation loss
    plt.plot(train_losses, label='Train')
    plt.plot(valid_losses, label='Validation')
    plt.title('Training and Validation Loss')
    plt.xlabel('Epoch')
    plt.ylabel('Loss')
    plt.legend()
    plt.show()

    # Plotting training and validation accuracy
    plt.plot(train_acc, label='Train')
    plt.plot(valid_acc, label='Validation')
    plt.title('Training and Validation Accuracy')
    plt.xlabel('Epoch')
    plt.ylabel('Accuracy')
    plt.legend()
    plt.show()

train_iter = torchtext.data.BucketIterator(train,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

val_iter = torchtext.data.BucketIterator(valid,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

model = RNN(input_dim=len(text_field.vocab),
            hidden_dim=50,
            output_dim=2,
            num_layers=1)

train_rnn_network(model, train_iter, val_iter, num_epochs=5, learning_rate=1e-4)

"""### Part (c) [4 pt]

Choose at least 4 hyperparameters to tune. Explain how you tuned the hyperparameters.
You don't need to include your training curve for every model you trained.
Instead, explain what hyperparemters you tuned, what the best validation accuracy was,
and the reasoning behind the hyperparameter decisions you made.

For this assignment, you should tune more than just your learning rate and epoch.
Choose at least 2 hyperparameters that are unrelated to the optimizer.
"""

# Firstly, I choose 100 hidden units
train_iter = torchtext.data.BucketIterator(train,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

val_iter = torchtext.data.BucketIterator(valid,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

model = RNN(input_dim=len(text_field.vocab),
            hidden_dim=100,
            output_dim=2,
            num_layers=1)

train_rnn_network(model, train_iter, val_iter, num_epochs=5, learning_rate=1e-4)

"""In tuning the hyperparameters for the RNN model, I focused initially on the hidden dimension size (`hidden_dim`). The reasoning was that a larger hidden dimension could potentially allow the model to learn more complex patterns. I increased `hidden_dim` from 50 to 100 and observed the impact on training and validation performance. The best validation accuracy achieved with this change was 95.78% in the fourth epoch. This improvement in validation accuracy, compared to the original configuration, indicated that the increased model capacity (due to a larger hidden dimension) helped in capturing the complexities of the dataset better. However, I also noted fluctuations in validation accuracy, suggesting the need for further exploration, possibly by adjusting other hyperparameters like the learning rate, number of layers, or batch size. The choice of this hyperparameter was driven by the hypothesis that a more complex model could perform better on the dataset, which seemed to be validated by the results."""

# Secondly, adjusting number of RNN layers to 2
train_iter = torchtext.data.BucketIterator(train,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

val_iter = torchtext.data.BucketIterator(valid,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

model = RNN(input_dim=len(text_field.vocab),
            hidden_dim=100,  # Keeping hidden_dim as 100 from the first tuning
            output_dim=2,
            num_layers=2)  # Increased from 1 to 2 layers

train_rnn_network(model, train_iter, val_iter, num_epochs=5, learning_rate=1e-4)

"""After increasing the number of RNN layers from 1 to 2, there were notable changes in the model's performance. Initially, the model showed a decrease in both training and validation accuracy, with particularly low validation accuracy in the first two epochs. However, from the third epoch onwards, there was a significant improvement. By the fifth epoch, the model achieved a training accuracy of 95.68% and a validation accuracy of 96.05%, which is an improvement compared to the single-layer model.

This pattern suggests that the additional layer allowed the model to capture more complex features in the data, which contributed to the higher accuracy. However, the initial drop in performance could indicate that the model required more epochs to effectively learn from the data due to its increased complexity. The final high validation accuracy is a positive sign that the model was able to generalize well, despite the complexity added by the extra layer.

The results imply that while adding layers can increase the model's learning capacity, it may also require adjustments in other training aspects, such as the number of epochs or learning rate, to fully realize the benefits of a deeper network.
"""

# Thirdly, change learning_rate to 5e^-4
train_iter = torchtext.data.BucketIterator(train,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

val_iter = torchtext.data.BucketIterator(valid,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

model = RNN(input_dim=len(text_field.vocab),
            hidden_dim=100,
            output_dim=2,
            num_layers=2)

# Experiment with a different learning rate, for example, 5e-4
train_rnn_network(model, train_iter, val_iter, num_epochs=5, learning_rate=5e-4)

"""Adjusting the learning rate to `5e-4` yielded significant improvements in both training and validation performance of the RNN model. The training process showed a consistent decrease in loss and increase in accuracy across epochs, indicating effective learning. Notably, the model achieved a high validation accuracy of 97.76% by the fifth epoch, which is a substantial improvement compared to previous configurations.

This increase in the learning rate seems to have facilitated faster convergence without sacrificing the model's ability to generalize, as evidenced by the high validation accuracy. The results suggest that the previous learning rate might have been too conservative, and the model benefited from the increased rate to navigate the optimization landscape more effectively.

The success with this adjustment highlights the importance of the learning rate in training dynamics. It underscores the need for a balanced approach where the learning rate is neither too high (causing instability) nor too low (slowing down convergence), especially in complex models like RNNs.
"""

# Fourthly, experimenting with a different number of epoch 10
train_iter = torchtext.data.BucketIterator(train,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

val_iter = torchtext.data.BucketIterator(valid,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

model = RNN(input_dim=len(text_field.vocab),
            hidden_dim=100,
            output_dim=2,
            num_layers=2)

train_rnn_network(model, train_iter, val_iter, num_epochs=10, learning_rate=5e-4)

"""Extending the training to 10 epochs significantly enhanced the performance of the RNN model. The training process exhibited a steady decrease in loss and a consistent increase in accuracy, indicating effective learning. Notably, the validation accuracy improved markedly, peaking at 98.39% in the final epoch. This suggests that the extended training duration allowed the model to better understand and generalize the data, without signs of overfitting. The close alignment of training and validation accuracies throughout the training indicates a balanced model in terms of learning capacity and generalization ability. The result underscores the importance of sufficient training duration for complex models, especially when dealing with nuanced datasets."""

torch.save(model.state_dict(), "/content/final_model")

"""### Part (d) [2 pt]

Before we deploy a machine learning model, we usually want to have a better understanding
of how our model performs beyond its validation accuracy. An important metric to track is
*how well our model performs in certain subsets of the data*.

In particular, what is the model's error rate amongst data with negative labels?
This is called the **false positive rate**.

What about the model's error rate amongst data with positive labels?
This is called the **false negative rate**.

Report your final model's false positive and false negative rate across the
validation set.
"""

# Create a Dataset of only spam validation examples
valid_spam = torchtext.data.Dataset(
    [e for e in valid.examples if e.label == 1],
    valid.fields)
# Create a Dataset of only non-spam validation examples
valid_nospam = torchtext.data.Dataset(
    [e for e in valid.examples if e.label == 0],
    valid.fields)

valid_spam_iter = torchtext.data.BucketIterator(valid_spam,
                            batch_size=32,
                            sort_key=lambda x: len(x.text),
                            sort_within_batch=True,
                            repeat=False)

valid_nospam_iter = torchtext.data.BucketIterator(valid_nospam,
                              batch_size=32,
                              sort_key=lambda x: len(x.text),
                              sort_within_batch=True,
                              repeat=False)

best_model = RNN(input_dim=len(text_field.vocab),
                 hidden_dim=100,
                 output_dim=2,
                 num_layers=2)

state = torch.load("final_model")  # Load the saved model state
best_model.load_state_dict(state)

fpr = 1 - get_accuracy(best_model, valid_nospam_iter)
fnr = 1 - get_accuracy(best_model, valid_spam_iter)

print(f"The model's false positive rate is {fpr:.4f}.")
print(f"The model's false negative rate is {fnr:.4f}.")

"""### Part (e) [2 pt]

The impact of a false positive vs a false negative can be drastically different.
If our spam detection algorithm was deployed on your phone, what is the impact
of a false positive on the phone's user? What is the impact of a false negative?

In a spam detection algorithm deployed on a phone, the impact of a false positive (legitimate messages incorrectly marked as spam) primarily includes inconvenience and potential missed important communications, leading to disruptions in personal or professional exchanges. On the other hand, false negatives (spam messages misclassified as legitimate) primarily result in exposure to unwanted, potentially harmful content, posing security risks and decreasing productivity due to the need to manually filter spam. Therefore, striking a balance between minimizing false positives and false negatives is crucial to ensure both the security and efficiency of communication for the user.

## Part 4. Evaluation [11 pt]

### Part (a) [1 pt]

Report the final test accuracy of your model.
"""

test_iter = torchtext.data.BucketIterator(test,
                          batch_size=32,
                          sort_key=lambda x: len(x.text),
                          sort_within_batch=True,
                          repeat=False)

final_test_accuracy = get_accuracy(best_model, test_iter)
print(f"Final Test Accuracy: {final_test_accuracy:.4f}")

"""### Part (b) [3 pt]

Report the false positive rate and false negative rate of your model across the test set.
"""

# Create a Dataset of only spam test examples
test_spam = torchtext.data.Dataset(
    [e for e in test.examples if e.label == 1],
    test.fields)
# Create a Dataset of only non-spam test examples
test_nospam = torchtext.data.Dataset(
    [e for e in test.examples if e.label == 0],
    test_spam.fields)

test_spam_iter = torchtext.data.BucketIterator(test_spam,
                            batch_size=32,
                            sort_key=lambda x: len(x.text),
                            sort_within_batch=True,
                            repeat=False)

test_nospam_iter = torchtext.data.BucketIterator(test_nospam,
                              batch_size=32,
                              sort_key=lambda x: len(x.text),
                              sort_within_batch=True,
                              repeat=False)

fpr = 1 - get_accuracy(best_model, test_nospam_iter)
fnr = 1 - get_accuracy(best_model, test_spam_iter)

print(f"The model's false positive rate is {fpr:.4f}.")
print(f"The model's false negative rate is {fnr:.4f}.")

"""### Part (c) [3 pt]

What is your model's prediction of the **probability** that
the SMS message "machine learning is sooo cool!" is spam?

Hint: To begin, use `text_field.vocab.stoi` to look up the index
of each character in the vocabulary.
"""

msg = "machine learning is sooo cool!"
indexed_msg = [text_field.vocab.stoi[char] for char in msg]

msg_tensor = torch.LongTensor(indexed_msg).unsqueeze(0)

# Pass the message through the model
model.eval()  # Set the model to evaluation mode
with torch.no_grad():
    prediction_logits = model(msg_tensor)[0]

# Apply softmax to get probabilities
probabilities = torch.softmax(prediction_logits, dim=0)

# The probability of the message being spam
spam_probability = probabilities[1].item()
print(f"Probability that the message is spam: {spam_probability:.4f}")

"""### Part (d) [4 pt]

Do you think detecting spam is an easy or difficult task?

Since machine learning models are expensive to train and deploy, it is very
important to compare our models against baseline models: a simple
model that is easy to build and inexpensive to run that we can compare our
recurrent neural network model against.

Explain how you might build a simple baseline model. This baseline model
can be a simple neural network (with very few weights), a hand-written algorithm,
or any other strategy that is easy to build and test.

**Do not actually build a baseline model. Instead, provide instructions on
how to build it.**

Detecting spam can be a challenging task due to the ever-evolving nature of spam content, the subtlety of language, and the need for precision in distinguishing spam from legitimate messages. A simple and effective baseline model for comparison with more complex machine learning models is a Naive Bayes Classifier.

### Building a Naive Bayes Classifier as a Baseline Model:

1. **Feature Extraction**:
   - Utilize a Bag-of-Words model to convert SMS messages into numerical data. This involves representing each message by the frequency or presence of words in it.

2. **Data Preparation**:
   - Split your dataset into training and testing sets. Preprocess the text by lowercasing, removing punctuation, and tokenizing.

3. **Naive Bayes Algorithm**:
   - Implement or use an existing Naive Bayes classifier. This involves calculating the conditional probabilities of each word given a spam/non-spam label and the overall probability of spam/non-spam messages in your training set.

4. **Prediction**:
   - For a new message, the classifier calculates the likelihood of it being spam or non-spam based on the learned probabilities and classifies it into the category with the higher likelihood.

5. **Implementation Tools**:
   - Use libraries like Scikit-learn in Python, which provide straightforward implementations of Naive Bayes.
   
In conclusion, a Naive Bayes classifier offers a practical and efficient baseline model for spam detection. Its simplicity and effectiveness make it a suitable choice for benchmarking against more sophisticated machine learning models like RNNs.
"""

# Commented out IPython magic to ensure Python compatibility.
# %%shell
# jupyter nbconvert --to html "/content/Lab5_Spam_Detection.ipynb"